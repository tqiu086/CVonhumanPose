import cv2
from ultralytics import YOLO
from modules.which_pose import classify_pose
import time
import json
import os
import requests

# wasn't sure
import numpy

# å­˜å‚¨æ•°å­—åºåˆ—å’Œå®šæ—¶å™¨
sequence = ""
last_saved_time = 0
output_path = r"backend/checkpassword.json"

def draw_simplified_pose(image, keypoints, confidence_threshold=0.5):
    """
    åœ¨è§†é¢‘å¸§ä¸­ç»˜åˆ¶ç®€åŒ–ç‰ˆäººä½“éª¨æ¶ï¼Œåªå…³æ³¨å¤´ã€è‚©ã€æ‰‹è‡‚ã€èº¯å¹²å’Œè…¿éƒ¨ã€‚

    Args:
        image (np.array): æ‘„åƒå¤´è¾“å…¥å›¾åƒã€‚
        keypoints (list): å…³é”®ç‚¹åˆ—è¡¨ [x, y, confidence]ã€‚
        confidence_threshold (float): å…³é”®ç‚¹å¯è§†åŒ–ç½®ä¿¡åº¦é˜ˆå€¼ã€‚
    """
    if keypoints is None or len(keypoints) == 0:
        return  
    skeleton = [
        (5, 7), (7, 9),      # å·¦è‡‚
        (6, 8), (8, 10),     # å³è‡‚
        (11, 13), (13, 15),  # å·¦è…¿
        (12, 14), (14, 16),  # å³è…¿
        (5, 6),              # è‚©è¿çº¿
        (11, 12),            # é«‹è¿çº¿
        (5, 11), (6, 12),    # è‚©åˆ°é«‹ï¼Œæ„å»ºèº¯å¹²
        (0, 5), (0, 6)       # é¼»å­åˆ°è‚©ï¼ˆå¤´éƒ¨è¿çº¿ï¼‰
    ]

    for person in keypoints:
        # ç»˜åˆ¶å…³é”®ç‚¹
        for i, keypoint in enumerate(person):
            x, y, conf = keypoint
            if conf > confidence_threshold:
                cv2.circle(image, (int(x), int(y)), 5, (0, 255, 0), -1)

        # è¿æ¥éª¨æ¶
        for start_idx, end_idx in skeleton:
            if (person[start_idx][2] > confidence_threshold and person[end_idx][2] > confidence_threshold):
                x1, y1 = int(person[start_idx][0]), int(person[start_idx][1])
                x2, y2 = int(person[end_idx][0]), int(person[end_idx][1])
                cv2.line(image, (x1, y1), (x2, y2), (255, 0, 0), 2)


def draw_custom_pose(image, keypoints, confidence_threshold=0.3):
    """
    ç»˜åˆ¶å®šåˆ¶éª¨æ¶ï¼šåŠ å…¥ neck å’Œ pelvis ä¸¤ä¸ªä¸­ç‚¹ï¼Œè¿æ¥æ›´åˆç†ã€‚

    Args:
        image (np.array): å›¾åƒã€‚
        keypoints (list): æ¯äºº [17, 3] çš„å…³é”®ç‚¹åˆ—è¡¨ã€‚
        confidence_threshold (float): æœ€å°ç½®ä¿¡åº¦ã€‚
    """
    if keypoints is None or len(keypoints) == 0:
        return  
    skeleton = [
        (1, 2),      # å·¦çœ¼ - å³çœ¼s
        (0, 17),     # é¼»å­ - neck
        (5, 7), (7, 9),    # å·¦è‡‚
        (6, 8), (8, 10),   # å³è‡‚
        (17, 5), (17, 6),  # neck - è‚©
        (18, 11), (18, 12),# pelvis - é«‹
        (11, 13), (13, 15),# å·¦è…¿
        (12, 14), (14, 16),# å³è…¿
        (17, 18)           # neck - pelvis
    ]

    for person in keypoints:
        keypts = person.tolist()

        # -- æ„é€  neck --
        if person[5][2] > confidence_threshold and person[6][2] > confidence_threshold:
            neck = [
                (person[5][0] + person[6][0]) / 2,
                (person[5][1] + person[6][1]) / 2,
                (person[5][2] + person[6][2]) / 2
            ]
        else:
            neck = [0, 0, 0]

        # -- æ„é€  pelvis --
        if person[11][2] > confidence_threshold and person[12][2] > confidence_threshold:
            pelvis = [
                (person[11][0] + person[12][0]) / 2,
                (person[11][1] + person[12][1]) / 2,
                (person[11][2] + person[12][2]) / 2
            ]
        else:
            pelvis = [0, 0, 0]

        # æ·»åŠ ä¸¤ä¸ªä¸­ç‚¹åˆ°å…³é”®ç‚¹åˆ—è¡¨ï¼ˆä½œä¸ºç´¢å¼• 17ã€18ï¼‰
        keypts.append(neck)    # index 17
        keypts.append(pelvis)  # index 18

        # ç»˜åˆ¶æ‰€æœ‰å…³é”®ç‚¹
        for x, y, conf in keypts:
            if conf > confidence_threshold:
                cv2.circle(image, (int(x), int(y)), 4, (0, 255, 0), -1)

        # ç»˜åˆ¶éª¨æ¶çº¿
        for i, j in skeleton:
            if keypts[i][2] > confidence_threshold and keypts[j][2] > confidence_threshold:
                x1, y1 = int(keypts[i][0]), int(keypts[i][1])
                x2, y2 = int(keypts[j][0]), int(keypts[j][1])
                cv2.line(image, (x1, y1), (x2, y2), (255, 0, 255), 2)

def action_to_digit(action: str) -> str:
    return {
        "Left Hand Up": "1",
        "Right Hand Up": "2",
        "Both Hands Up": "3",
        "Arms Sideways": "4"
    }.get(action, "")


def real_time_pose_estimation(camera_index=0, model_path='yolov8l-pose.pt', confidence_threshold=0.7):
    """
    å®æ—¶æ‘„åƒå¤´äººä½“å§¿æ€æ£€æµ‹ã€‚

    Args:
        camera_index (int): æ‘„åƒå¤´ç´¢å¼•ï¼ˆé»˜è®¤ 0ï¼‰ã€‚
        model_path (str): YOLOv8-Pose æ¨¡å‹è·¯å¾„ã€‚
        confidence_threshold (float): å…³é”®ç‚¹ç½®ä¿¡åº¦é˜ˆå€¼ã€‚
    """
    print(f"ğŸ¥ æ­£åœ¨æ‰“å¼€æ‘„åƒå¤´ç´¢å¼• {camera_index}...")

    # åŠ è½½ YOLOv8-Pose æ¨¡å‹
    model = YOLO(model_path)
    print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹: {model_path}")

    # æ‰“å¼€æ‘„åƒå¤´
    cap = cv2.VideoCapture(camera_index)
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)  # è®¾ç½®åˆ†è¾¨ç‡
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)

    if not cap.isOpened():
        print("âŒ é”™è¯¯: æ— æ³•æ‰“å¼€æ‘„åƒå¤´ï¼")
        return
    start_time = time.time()

    print("ğŸš€ æŒ‰ 'q' é€€å‡º...")

    while True:
        ret, frame = cap.read()
        if not ret:
            print("âŒ é”™è¯¯: è¯»å–æ‘„åƒå¤´å¸§å¤±è´¥ï¼")
            break

        # æ£€æµ‹äººä½“å§¿æ€
        results = model(frame, conf=confidence_threshold)
        keypoints = results[0].keypoints.data.cpu().numpy()  # æå–å…³é”®ç‚¹
        if keypoints.shape[0] > 0:
            draw_custom_pose(frame, keypoints)

        current_time = time.time()
        global sequence, last_saved_time
        current_time = time.time()

        # ç­‰å¾… 3 ç§’é’Ÿå†å¼€å§‹è®°å½•åŠ¨ä½œ
        if current_time - start_time < 5:
            cv2.putText(frame, f"â³ Starting in {5 - int(current_time - start_time)} sec...",
                        (20, 100), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 2)
            cv2.imshow("å®æ—¶å§¿æ€æ£€æµ‹", frame)
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
            continue
        for person in keypoints:
            action_label = classify_pose(person)
            digit = action_to_digit(action_label)

            # è·å–é¼»å­ä½ç½®ç”¨äºæ˜¾ç¤º
            nose = person[0]
            if nose[2] > confidence_threshold:
                cv2.putText(frame, action_label, (int(nose[0]), int(nose[1]) - 10),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)

            # å¦‚æœè¯†åˆ«åˆ°æ–°åŠ¨ä½œå¹¶è¶…è¿‡ 2 ç§’é—´éš”ï¼Œè®°å½•æ•°å­—
            if digit and (current_time - last_saved_time >= 2) and len(sequence) < 4:
                sequence += digit
                last_saved_time = current_time
                print(f"âœ… å·²æ·»åŠ åŠ¨ä½œï¼š{action_label} â†’ å½“å‰åºåˆ—ï¼š{sequence}")

        # åœ¨å·¦ä¸Šè§’æ˜¾ç¤ºå½“å‰å·²è®°å½•æ•°å­—åºåˆ—
        cv2.putText(frame, f"Password: {sequence}", (20, 40),
                    cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 0), 2)
        # âœ… æ˜¾ç¤ºè§†é¢‘å¸§
        cv2.imshow("å®æ—¶å§¿æ€æ£€æµ‹", frame)

        # å¦‚æœæŒ‰ä¸‹ q é”®ä¹Ÿå¯ä»¥é€€å‡º
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

        # ä¸€æ—¦è®°å½•æ»¡ 4 ä¸ªæ•°å­—ï¼Œå†™å…¥ JSON å¹¶ç­‰å¾…å…³é—­
        if len(sequence) == 4:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump({"password": sequence}, f, ensure_ascii=False)
                print(f"ğŸ’¾ å†™å…¥ JSON æˆåŠŸ: {output_path}")
            SERVER_IP = "http://10.186.9.149:5000/upload"
            # è¦å‘é€çš„ checkpassword å†…å®¹
            data = {
                "password": sequence
            }

            try:
                response = requests.post(SERVER_IP, json=data)
                print(f"Server response: {response.status_code} - {response.text}")
            except Exception as e:
                print(f"Error sending request: {e}")
            time.sleep(2)

            
            break  # é€€å‡ºæ‘„åƒå¤´



if __name__ == "__main__":
    real_time_pose_estimation()
