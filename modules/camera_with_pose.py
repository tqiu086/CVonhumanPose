import cv2
from ultralytics import YOLO

def draw_simplified_pose(image, keypoints, confidence_threshold=0.3):
    """
    åœ¨è§†é¢‘å¸§ä¸­ç»˜åˆ¶ç®€åŒ–ç‰ˆäººä½“éª¨æ¶ï¼Œåªå…³æ³¨å¤´ã€è‚©ã€æ‰‹è‡‚ã€èº¯å¹²å’Œè…¿éƒ¨ã€‚

    Args:
        image (np.array): æ‘„åƒå¤´è¾“å…¥å›¾åƒã€‚
        keypoints (list): å…³é”®ç‚¹åˆ—è¡¨ [x, y, confidence]ã€‚
        confidence_threshold (float): å…³é”®ç‚¹å¯è§†åŒ–ç½®ä¿¡åº¦é˜ˆå€¼ã€‚
    """
    skeleton = [
        (5, 7), (7, 9),      # å·¦è‡‚
        (6, 8), (8, 10),     # å³è‡‚
        (11, 13), (13, 15),  # å·¦è…¿
        (12, 14), (14, 16),  # å³è…¿
        (5, 6),              # è‚©è¿çº¿
        (11, 12),            # é«‹è¿çº¿
        (5, 11), (6, 12),    # è‚©åˆ°é«‹ï¼Œæ„å»ºèº¯å¹²
        (0, 5), (0, 6)       # é¼»å­åˆ°è‚©ï¼ˆå¤´éƒ¨è¿çº¿ï¼‰
    ]

    for person in keypoints:
        # ç»˜åˆ¶å…³é”®ç‚¹
        for i, keypoint in enumerate(person):
            x, y, conf = keypoint
            if conf > confidence_threshold:
                cv2.circle(image, (int(x), int(y)), 5, (0, 255, 0), -1)

        # è¿æ¥éª¨æ¶
        for start_idx, end_idx in skeleton:
            if (person[start_idx][2] > confidence_threshold and person[end_idx][2] > confidence_threshold):
                x1, y1 = int(person[start_idx][0]), int(person[start_idx][1])
                x2, y2 = int(person[end_idx][0]), int(person[end_idx][1])
                cv2.line(image, (x1, y1), (x2, y2), (255, 0, 0), 2)


def draw_custom_pose(image, keypoints, confidence_threshold=0.3):
    """
    ç»˜åˆ¶ç¬¦åˆè‡ªå®šä¹‰ç»“æ„çš„éª¨æ¶å›¾ï¼Œæ·»åŠ â€œè„–å­ä¸­ç‚¹â€è¿æ¥ã€‚

    Args:
        image (np.array): åŸå›¾åƒã€‚
        keypoints (list): å…³é”®ç‚¹åæ ‡ [17, 3]ã€‚
        confidence_threshold (float): å¯è§†åŒ–é˜ˆå€¼ã€‚
    """
    custom_skeleton = [
        (1, 2),                 # çœ¼ç›
        (5, 6),                 # è‚©è†€
        (11, 12),               # é«‹éƒ¨
        (5, 7), (7, 9),         # å·¦è‡‚
        (6, 8), (8, 10),        # å³è‡‚
        (11, 13), (13, 15),     # å·¦è…¿
        (12, 14), (14, 16)      # å³è…¿
    ]

    for person in keypoints:
        # æ„é€  neck ä¸­ç‚¹
        if person[5][2] > confidence_threshold and person[6][2] > confidence_threshold:
            neck_x = (person[5][0] + person[6][0]) / 2
            neck_y = (person[5][1] + person[6][1]) / 2
            neck_conf = (person[5][2] + person[6][2]) / 2
            neck = [neck_x, neck_y, neck_conf]
        else:
            neck = None

        # ç»˜åˆ¶å…³é”®ç‚¹
        for i, kp in enumerate(person):
            x, y, conf = kp
            if conf > confidence_threshold:
                cv2.circle(image, (int(x), int(y)), 4, (0, 255, 0), -1)

        # ç»˜åˆ¶è‡ªå®šä¹‰éª¨æ¶
        for start_idx, end_idx in custom_skeleton:
            if person[start_idx][2] > confidence_threshold and person[end_idx][2] > confidence_threshold:
                x1, y1 = int(person[start_idx][0]), int(person[start_idx][1])
                x2, y2 = int(person[end_idx][0]), int(person[end_idx][1])
                cv2.line(image, (x1, y1), (x2, y2), (255, 0, 0), 2)

        # ç»˜åˆ¶é¢å¤–çš„ neck éª¨æ¶
        if neck is not None:
            for idx in [5, 6, 11, 12]:
                if person[idx][2] > confidence_threshold:
                    x1, y1 = int(neck[0]), int(neck[1])
                    x2, y2 = int(person[idx][0]), int(person[idx][1])
                    cv2.line(image, (x1, y1), (x2, y2), (0, 255, 255), 2)
            cv2.circle(image, (int(neck[0]), int(neck[1])), 4, (0, 128, 255), -1)  # neck ç‚¹


def real_time_pose_estimation(camera_index=0, model_path='yolov8n-pose.pt', confidence_threshold=0.3):
    """
    å®æ—¶æ‘„åƒå¤´äººä½“å§¿æ€æ£€æµ‹ã€‚

    Args:
        camera_index (int): æ‘„åƒå¤´ç´¢å¼•ï¼ˆé»˜è®¤ 0ï¼‰ã€‚
        model_path (str): YOLOv8-Pose æ¨¡å‹è·¯å¾„ã€‚
        confidence_threshold (float): å…³é”®ç‚¹ç½®ä¿¡åº¦é˜ˆå€¼ã€‚
    """
    print(f"ğŸ¥ æ­£åœ¨æ‰“å¼€æ‘„åƒå¤´ç´¢å¼• {camera_index}...")

    # åŠ è½½ YOLOv8-Pose æ¨¡å‹
    model = YOLO(model_path)
    print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹: {model_path}")

    # æ‰“å¼€æ‘„åƒå¤´
    cap = cv2.VideoCapture(camera_index)
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)  # è®¾ç½®åˆ†è¾¨ç‡
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)

    if not cap.isOpened():
        print("âŒ é”™è¯¯: æ— æ³•æ‰“å¼€æ‘„åƒå¤´ï¼")
        return

    print("ğŸš€ æŒ‰ 'q' é€€å‡º...")

    while True:
        ret, frame = cap.read()
        if not ret:
            print("âŒ é”™è¯¯: è¯»å–æ‘„åƒå¤´å¸§å¤±è´¥ï¼")
            break

        # æ£€æµ‹äººä½“å§¿æ€
        results = model(frame, conf=confidence_threshold)
        keypoints = results[0].keypoints.data.cpu().numpy()  # æå–å…³é”®ç‚¹

        # ç»˜åˆ¶ç«æŸ´äººéª¨æ¶
        draw_custom_pose(frame, keypoints)

        # æ˜¾ç¤ºç»“æœ
        cv2.imshow("å®æ—¶å§¿æ€æ£€æµ‹", frame)

        # æŒ‰ 'q' é”®é€€å‡º
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    # é‡Šæ”¾èµ„æº
    cap.release()
    cv2.destroyAllWindows()
    print("âœ… é€€å‡ºç¨‹åºï¼Œæ‘„åƒå¤´å·²é‡Šæ”¾ã€‚")


if __name__ == "__main__":
    real_time_pose_estimation()
